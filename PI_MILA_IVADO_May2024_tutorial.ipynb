{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Endgoal: Study Generalization Error of a Linearized Transformer architecture for in-context learning (ICL) tasks.**\n"
      ],
      "metadata": {
        "id": "t4M4MbdaTTXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Q. What is Transformer?**\n",
        "\n",
        "Ans. Transformer is a NN architecture, used as the backbone of Large Language Models (LLM). What makes Transformers special in comparison to other NN architectures is a layer called the *Attention Mechanism*. This layer can store correlations between different parts of the incoming data, through matrix operations. Attention mechanism equips Transformer architectures with a capability to predict\n",
        "1. missing parts of the data it has been trained on,\n",
        "2. as well as missing parts of new, unseen data of similar type.\n",
        "\n",
        "This property makes LLMs extremely efficient in learning various languages through hidden structures and syntaxes. As a result, LLMs keep showing groundbreaking performance when it comes to predicting text outputs.\n",
        "\n",
        "****\n",
        "\n",
        "Given below is the Transformer architecture taken from the original reference paper __[Attention is all you need](https://arxiv.org/pdf/1706.03762)__.\n",
        "><div>\n",
        "><img src=\"https://drive.google.com/uc?export=view&id=17btW9mybtoC3BkdNVXiszCruFz6x6jmO\" width=\"400\"/>\n",
        "></div>\n",
        "\n",
        "****\n",
        "\n",
        "Pytorch has an inbuilt class called *Transformer*. One can directly call this using the code snippet ```torch.nn.Transformer```.\n",
        "\n",
        "See __[documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)__.\n",
        "\n",
        "\n",
        "# **Q. What is in-context learning (ICL)?**\n",
        "\n",
        "Ans. In a nut shell, ICL is the process of providing a prompt or “context” to a pre-trained LLM to guide its answers for a specific task. It is a specific kind of prompt engineering where the demonstrations of the task are included as part of the model. Let us understand this using a single example.\n",
        "\n",
        "Consider a prompt made of $l+1$ tokens: a sequence $\\{x_1,x_2,\\cdots , x_l,x_{l+1}\\}$. We want to train a Transformer architecture such that it produces outputs $\\{y_1,y_2,\\cdots,y_l,y_{l+1}\\}$, when this input sequence is provided. ICL would require training the Transformer architecture on a subset of the input and target data $\\{x_1,y_1,\\cdots, x_n,y_n\\}$. With that, the trained Transformer would be able to predict correct target $y_p$ for input tokens $x_p$ that were not included the training set.\n",
        "\n",
        "# Real-life example in-context learning in LLMs:\n",
        "\n",
        "**Inputs**\n",
        ">Example 1:\n",
        ">Description: This animal has a long trunk and large ears. \\\\\n",
        ">Animal: Elephant\n",
        ">\n",
        ">Example 2:\n",
        ">Description: This animal has a mane and is known as the king of the jungle. \\\\\n",
        ">Animal: Lion\n",
        ">\n",
        ">Example 3:\n",
        ">Description: This animal has black and white stripes. \\\\\n",
        ">Animal: \\\\\n",
        "\n",
        "**Expected output**\n",
        ">Example 3:\n",
        ">Description: This animal has black and white stripes. \\\\\n",
        ">Animal: Zebra\n"
      ],
      "metadata": {
        "id": "Sn9L6cUQVKB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time to build a Transformer and experiment with ICL.\n",
        "\n",
        "**But we don't have sufficient time+compute to build a full Transformer from scratch and train it. Let's simplify the architecture.**\n",
        "\n",
        "> # Let us build a Linearized Transformer, where the softmax structure of Attention is replaced with a linearized form.\n",
        "\n",
        "Attention layer in Transformers are made of three matrices: key $K$, query $Q$, and value $V$.\n",
        "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Here, $d_k$ is the dimension of the Key matrix, used to normalize the exponents within softmax function. There are several literature that explains how key, query, and values are constructed using parameter matrices for $K$, $Q$, $V$, and data $\\vec{x},\\vec{y}$, e.g. __[this paper](https://arxiv.org/pdf/2304.07235)__.\n",
        "\n",
        "We will simplify life in this session by choosing a linearized form of attention layer which is shown to closely replicate the performances of actual transformers for ICL tasks, see references __[[1]](https://arxiv.org/abs/2402.14180)__, __[[2]](https://arxiv.org/abs/2306.09927)__. These results are particularly well established for last token predictions.\n",
        "\n",
        "Let us define the task: an input sequence made of input tokens $\\vec{x}_i$ and scalar targets $y_i$,\n",
        "$$Z = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_l & x_{l+1} \\\\ y_1 & y_2 & \\cdots & y_l & 0 \\end{bmatrix} \\in \\mathbb{R}^{(d+1)\\times(l+1)}.$$\n",
        "$d$ is the vector dimension of each input token, and $l$ is the number of tokens in a sequence. We will eventually consider $p$ such sequences or samples or prompts. Here $0$ is a token that prompts the target scalar $y_{l+1}$ to be predicted.  \n",
        "\n",
        "**Define a *Linearized Attention* layer**: For arbitrarily sized parameter matrices $K$, $Q$, $V$, the output of the linearized attention layer is the following\n",
        "$$\\text{Linearized Attention}(Q,K,V) = Z + \\frac{1}{l} V Z (K Z)^\\top (QZ).$$"
      ],
      "metadata": {
        "id": "VZlbqKsGcNQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1: Look up Pytorch documentation on matrix operations and NN parameterization (e.g. __[this](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)__). Using this, define an architecture with a single hidden layer with the linearized attention defined above. Inputs $Z$ and Outputs $Z + \\frac{1}{l} V Z (K Z)^\\top (QZ)$**.\n",
        "\n",
        "\n",
        ">Hint: initialize parameters in key, query, and values matrices as independent and identical Gaussian draws from $\\mathbf{N}(0,1/d)$. This initialization will help to reduce divergences in Generalization Error of ICL tasks, when token dimensions grow.\n",
        "\n",
        ">Conventions: $d$:= each token dimension, $l$:= sequence length, $p$:= sample size."
      ],
      "metadata": {
        "id": "4lu7BpTNpibt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xxgZP8hSb4eE"
      },
      "outputs": [],
      "source": [
        "# importing essential libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import os\n",
        "# from mpi4py import MPI\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing essential libraries, part 2\n",
        "\n",
        "import warnings\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, MultiheadAttention, ModuleList, Linear, LayerNorm\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Dropout\n",
        "from typing import Optional, Union, Callable, Any\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "B3mQpCPBb_5m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cudf-cu11 --extra-index-url=https://pypi.ngc.nvidia.com\n",
        "!rm -rf /usr/local/lib/python3.8/dist-packages/cupy*\n",
        "!pip install cupy-cuda11x"
      ],
      "metadata": {
        "id": "PM5lLdlncFYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt-get update\n",
        "! sudo apt-get install texlive-latex-recommended\n",
        "! sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended\n",
        "! wget http://mirrors.ctan.org/macros/latex/...\n",
        "! unzip type1cm.zip -d /tmp/type1cm\n",
        "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins\n",
        "! sudo mkdir /usr/share/texmf/tex/latex/type1cm\n",
        "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm\n",
        "! sudo texhash\n",
        "!apt install cm-super"
      ],
      "metadata": {
        "id": "PvCNJkdJcIRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Solution:**"
      ],
      "metadata": {
        "id": "Nv_Ai3oHrxTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the architecture with a single linearized attention layer is defined below.\n",
        "\n",
        "\n",
        "class SingleLayerTransformer(nn.Module):\n",
        "    def __init__(self, sample_size, seq_length, input_dim):\n",
        "        super(SingleLayerTransformer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # Define key, query, and values for this linear transformer\n",
        "        self.K = nn.Parameter(torch.randn(input_dim + 1, input_dim + 1))  # Key matrix initialized as random Gaussian i.i.d. draws\n",
        "        self.Q = nn.Parameter(torch.randn(input_dim + 1, input_dim + 1))  # Query matrix initialized as random Gaussian i.i.d. draws\n",
        "        self.V = nn.Parameter(torch.randn(input_dim + 1, input_dim + 1))  # Value matrix initialized as random Gaussian i.i.d. draws\n",
        "\n",
        "    def forward(self, Z, sample_size, seq_length, input_dim):\n",
        "        N = seq_length  # we will divide by sequence length, I have labelled this as \"l\" in theory section\n",
        "\n",
        "        Q = self.Q/np.sqrt(input_dim+1)  # Use the redefined query with Identity/d as covariance\n",
        "        K = self.K/np.sqrt(input_dim+1)  # Use the redefined key with Identity/d as covariance\n",
        "        V = self.V/np.sqrt(input_dim+1)  # Use the redefined value with Identity/d as covariance\n",
        "\n",
        "        # we use einsum to construct the network predictor step-by-step\n",
        "        pred1 = torch.einsum('de, mje -> mjd', K, Z).to('cuda') # we use the convention: sample_size, sequence_length, input_dim for every tensor's shape\n",
        "        Qtrans = Q.permute(1,0) # transpose of Q\n",
        "        Ztrans = Z.permute(0,2,1) # transpose of Z\n",
        "        pred1 = torch.einsum('cd, mjd -> mjc', Qtrans, pred1).to('cuda') # This generates: Q^T K Z\n",
        "        pred1 = torch.einsum('mci, mjc -> mij', Ztrans, pred1).to('cuda') # This generates: Z^T Q^T K Z\n",
        "        pred1 = torch.einsum('mib, mij -> mjb', Z, pred1).to('cuda') # This generates: Z Z^T Q^T K Z\n",
        "        pred1 = torch.einsum('ab,mjb -> mja', V, pred1).to('cuda') # This generates: V Z Z^T Q^T K Z\n",
        "\n",
        "        ypred = Z[:,-1,-1] + pred1[:,-1,-1] # this ensures that the prediction is the last element as a scalar from Z + (VZ)(QZ)^T(KZ)/N\n",
        "\n",
        "\n",
        "\n",
        "        return ypred\n"
      ],
      "metadata": {
        "id": "KrFd6dcEcJFb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 2: Consider the following relation between inputs and targets $y_i := \\sum_{j=1}^{d} \\omega_{j}x^{j}_{i} + \\epsilon_i$, where $x^j_i \\sim \\mathbf{N}(0,1/\\sqrt{d})$, $\\omega_j \\sim \\mathbf{N}(0,1)$ and $\\epsilon_i \\sim \\mathbf{N}(0, 0.01)$. Using this, define a MSE or $L_2$ loss function for the Linearized Transformer. Then, write a function that (a) trains this architecture using combinations of various optimizers, e.g. __[Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)__, __[SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)__, until loss falls below $10^{-10}$, and then (b) computes test loss for a new sample of sequences**.\n",
        "\n",
        "> Hint: look up pytorch forward pass, backward pass documentations."
      ],
      "metadata": {
        "id": "Uk5eY4UCuLL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Solution:**"
      ],
      "metadata": {
        "id": "B9Na0K2217ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TestLoss(sample_size, seq_length, input_dim):\n",
        "\n",
        "    # define inputs, outputs, and target dataset\n",
        "    X = torch.randn(sample_size, seq_length+1, input_dim)/np.sqrt(input_dim)\n",
        "    omega = torch.randn(sample_size,input_dim)\n",
        "\n",
        "    Y = torch.einsum('ka,kia -> ki',omega,X) + 0.1*torch.randn(sample_size, seq_length+1) # covariance of Gaussian noise is 0.01\n",
        "    Ytarget = Y[:,-1].clone().detach().to('cuda')\n",
        "    Y[:,-1]=0\n",
        "    Z = torch.cat((X, Y.unsqueeze(dim=2)), dim=2).to('cuda')\n",
        "    test_loss = 0\n",
        "\n",
        "    # generate linear transformer's predictor and its MSE loss with respect to target\n",
        "    transformer = SingleLayerTransformer(sample_size, seq_length, input_dim)\n",
        "    transformer = transformer.to('cuda')\n",
        "    optimizer1 = optim.Adam(transformer.parameters(), lr=15/sample_size)  # Adjust learning rate as needed\n",
        "    optimizer2 =  optim.SGD(transformer.parameters(), lr=15/sample_size, momentum=0.9)\n",
        "\n",
        "    loss_prev = 1000\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer1.zero_grad()  # Zero the gradients to avoid accumulation\n",
        "        outputY = transformer(Z, sample_size, seq_length, input_dim)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function(outputY, Ytarget) # calculating loss\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer1.step()  # Update weights based on gradients\n",
        "\n",
        "        if (epoch % 1000 == 0) or epoch== epochs - 1:\n",
        "            dloss = np.abs(loss.item()-loss_prev)/loss_prev\n",
        "            print(f\"Seq_len {seq_length}, epoch {epoch+1}, Loss: {loss.item()}, deltaLoss: {dloss}\")\n",
        "            loss_prev = loss.item()\n",
        "\n",
        "\n",
        "            if (dloss < 1e-4) or loss.item() < 1e-10:\n",
        "                print(f\"Seq_len {seq_length}, epoch {epoch+1}, Loss: {loss.item()}, deltaLoss: {dloss}\")\n",
        "                break\n",
        "\n",
        "            if epoch== epochs - 1 and loss.item() > 1e-10:\n",
        "                for epoch1 in range(new_epochs):\n",
        "                    optimizer2.zero_grad()  # Zero the gradients to avoid accumulation\n",
        "                    outputY = transformer(Z, sample_size, seq_length, input_dim)  # Forward pass\n",
        "                    # Calculate loss\n",
        "                    loss = loss_function(outputY, Ytarget) # calculating loss again\n",
        "\n",
        "                    # Backpropagation\n",
        "                    loss.backward()  # Compute gradients\n",
        "                    optimizer2.step()  # Update weights based on gradients\n",
        "                    if (epoch1 % 1000 == 0) or epoch1== new_epochs - 1:\n",
        "                        dloss = np.abs(loss.item()-loss_prev)/loss_prev\n",
        "                        print(f\"Seq_len {seq_length}, epoch {epoch1+1}, Loss: {loss.item()}, deltaLoss: {dloss}\")\n",
        "                        loss_prev = loss.item()\n",
        "\n",
        "                    if (dloss < 1e-4) or loss.item() < 1e-10:\n",
        "                        print(f\"Seq_len {seq_length}, epoch {epoch1+1}, Loss: {loss.item()}, deltaLoss: {dloss}\")\n",
        "                        break\n",
        "\n",
        "\n",
        "\n",
        "    # compute empirical test loss on a new random data set X, Y\n",
        "    newX = torch.randn(test_sample_size, seq_length+1, input_dim)/np.sqrt(input_dim)\n",
        "    newomega = torch.randn(test_sample_size,input_dim)\n",
        "    newY = torch.einsum('ka,kia -> ki',newomega,newX) + 0.1*torch.randn(test_sample_size, seq_length+1) # covariance of Gaussian noise is 0.01\n",
        "    newYtarget = newY[:,-1].clone().detach().to('cuda')\n",
        "    newY[:,-1]=0\n",
        "    newZ = torch.cat((newX, newY.unsqueeze(dim=2)), dim=2).to('cuda')\n",
        "\n",
        "    with torch.no_grad():  # No need to calculate gradients during inference\n",
        "        newYpred = transformer(newZ, sample_size, seq_length, input_dim)\n",
        "        test_loss = loss_function(newYpred, newYtarget) # loss function between target y_{l+1} and the last token prediction given by the trained lienar transformer\n",
        "\n",
        "\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "Gl6kxZOEcTr8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercise 3: Define parameters $\\alpha = l/d$, $\\tau = p/d^2$. Then for $d=10$ and the following choices of $\\alpha, \\tau$, train the Linearized Transformer over prompt $Z$ from exercise:2 for $10,000$ epochs. Finally, compute test loss for a new sample size $1000$.**\n",
        "\n",
        "> Use $\\alpha=0.5$, \\\\\n",
        "> and $\\tau = [0.1, 0.3, 0.6, 0.8, 0.9, 1.0, 1.1, 1.2, 1.4, 1.6, 1.9, 2.2, 2.8]$."
      ],
      "metadata": {
        "id": "zDqoyjhGynyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Solution:**"
      ],
      "metadata": {
        "id": "tS3ZbyYX2AUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute test loss at various tau for fixed d and alpha.\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "epochs = 10000  # Choose the number of epochs\n",
        "new_epochs = 10000\n",
        "\n",
        "\n",
        "input_dim = 10\n",
        "\n",
        "test_sample_size = 1000\n",
        "\n",
        "\n",
        "tau_list = [0.1, 0.3, 0.6, 0.8, 0.9, 1.0, 1.1, 1.2, 1.4, 1.6, 1.9, 2.2, 2.8]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_loss_array = np.zeros((1, len(tau_list))) # tau values appear across columns, alpha values appear across rows\n",
        "\n",
        "\n",
        "alpha = 0.5\n",
        "seq_length = int(alpha*input_dim) # compute sequence length for each (alpha, tau) pair\n",
        "for j in range(len(tau_list)):\n",
        "    tau = tau_list[j]\n",
        "    sample_size = int(tau*(input_dim)**2) # compute sample size for each (alpha, tau) pair\n",
        "    testloss_ij = TestLoss(sample_size, seq_length, input_dim)\n",
        "    test_loss_array[0,j] = testloss_ij\n",
        "    print('done alpha, tau, test loss', alpha, tau, testloss_ij)\n",
        "\n",
        "print(test_loss_array)\n",
        "test_loss_list = list(test_loss_array[0,:])\n"
      ],
      "metadata": {
        "id": "4kjAxAA-ce3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364a5311-91c7-4fdc-d0c3-67f10e5e6bcd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq_len 5, epoch 1, Loss: 1.256969690322876, deltaLoss: 0.9987430303096771\n",
            "Seq_len 5, epoch 1001, Loss: 1.174865733128902e-13, deltaLoss: 0.9999999999999065\n",
            "Seq_len 5, epoch 1001, Loss: 1.174865733128902e-13, deltaLoss: 0.9999999999999065\n",
            "done alpha, tau, test loss 0.5 0.1 tensor(10.0565, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 3.555983066558838, deltaLoss: 0.9964440169334412\n",
            "Seq_len 5, epoch 1001, Loss: 2.9886353808405985e-12, deltaLoss: 0.9999999999991596\n",
            "Seq_len 5, epoch 1001, Loss: 2.9886353808405985e-12, deltaLoss: 0.9999999999991596\n",
            "done alpha, tau, test loss 0.5 0.3 tensor(2.1877, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 2.443218946456909, deltaLoss: 0.9975567810535431\n",
            "Seq_len 5, epoch 1001, Loss: 1.2330654719505674e-08, deltaLoss: 0.9999999949531111\n",
            "Seq_len 5, epoch 2001, Loss: 9.249630966223776e-05, deltaLoss: 7500.329959058806\n",
            "Seq_len 5, epoch 3001, Loss: 1.416283453181677e-06, deltaLoss: 0.9846882166612547\n",
            "Seq_len 5, epoch 4001, Loss: 0.003256660420447588, deltaLoss: 2298.4411275027674\n",
            "Seq_len 5, epoch 5001, Loss: 2.5140424853020704e-08, deltaLoss: 0.9999922803050956\n",
            "Seq_len 5, epoch 6001, Loss: 0.0007333284011110663, deltaLoss: 29168.292301078774\n",
            "Seq_len 5, epoch 7001, Loss: 1.1946100130444393e-05, deltaLoss: 0.9837097538942378\n",
            "Seq_len 5, epoch 8001, Loss: 0.00013474207662511617, deltaLoss: 10.279168528123144\n",
            "Seq_len 5, epoch 9001, Loss: 0.0002927033929154277, deltaLoss: 1.172323599626542\n",
            "Seq_len 5, epoch 10000, Loss: 0.0023359775077551603, deltaLoss: 6.980698428152852\n",
            "Seq_len 5, epoch 1, Loss: 0.0021147849038243294, deltaLoss: 0.09468952641731287\n",
            "Seq_len 5, epoch 1001, Loss: 3.8010017355816217e-10, deltaLoss: 0.9999998202653269\n",
            "Seq_len 5, epoch 1193, Loss: 9.982376036887786e-11, deltaLoss: 0.9999998202653269\n",
            "done alpha, tau, test loss 0.5 0.6 tensor(3.0904, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.110918641090393, deltaLoss: 0.9988890813589096\n",
            "Seq_len 5, epoch 1001, Loss: 2.757654874585569e-05, deltaLoss: 0.9999751768061802\n",
            "Seq_len 5, epoch 2001, Loss: 0.0015260105719789863, deltaLoss: 54.33725724863671\n",
            "Seq_len 5, epoch 3001, Loss: 0.0003874096437357366, deltaLoss: 0.7461291220064553\n",
            "Seq_len 5, epoch 4001, Loss: 2.698144498936017e-07, deltaLoss: 0.9993035422471888\n",
            "Seq_len 5, epoch 5001, Loss: 0.0001742710592225194, deltaLoss: 644.8922392453081\n",
            "Seq_len 5, epoch 6001, Loss: 3.0854902433929965e-05, deltaLoss: 0.8229487869553106\n",
            "Seq_len 5, epoch 7001, Loss: 0.0008140988065861166, deltaLoss: 25.384747394011622\n",
            "Seq_len 5, epoch 8001, Loss: 6.0748494433937594e-05, deltaLoss: 0.9253794576991416\n",
            "Seq_len 5, epoch 9001, Loss: 5.5386317399097607e-05, deltaLoss: 0.08826847619526136\n",
            "Seq_len 5, epoch 10000, Loss: 0.0004729488573502749, deltaLoss: 7.539091955551833\n",
            "Seq_len 5, epoch 1, Loss: 0.0005546219763346016, deltaLoss: 0.1726891136642245\n",
            "Seq_len 5, epoch 1001, Loss: 1.0808783201810002e-07, deltaLoss: 0.9998051144083174\n",
            "Seq_len 5, epoch 2001, Loss: 3.2698359575533686e-08, deltaLoss: 0.6974834357852776\n",
            "Seq_len 5, epoch 3001, Loss: 1.0258911053995234e-08, deltaLoss: 0.6862560939701883\n",
            "Seq_len 5, epoch 4001, Loss: 3.3165570290094593e-09, deltaLoss: 0.6767145156485339\n",
            "Seq_len 5, epoch 5001, Loss: 1.1184760895233126e-09, deltaLoss: 0.6627598802794106\n",
            "Seq_len 5, epoch 6001, Loss: 4.2190634319538844e-10, deltaLoss: 0.6227846557048866\n",
            "Seq_len 5, epoch 7001, Loss: 2.0451096371942867e-10, deltaLoss: 0.5152692842432144\n",
            "Seq_len 5, epoch 8001, Loss: 1.2793763271012892e-10, deltaLoss: 0.37442164281398493\n",
            "Seq_len 5, epoch 8786, Loss: 9.995944350027486e-11, deltaLoss: 0.37442164281398493\n",
            "done alpha, tau, test loss 0.5 0.8 tensor(3.6806, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.3941317796707153, deltaLoss: 0.9986058682203293\n",
            "Seq_len 5, epoch 1001, Loss: 6.396351818693802e-05, deltaLoss: 0.9999541194604988\n",
            "Seq_len 5, epoch 2001, Loss: 0.0029019799549132586, deltaLoss: 44.369298580981926\n",
            "Seq_len 5, epoch 3001, Loss: 0.0003716674109455198, deltaLoss: 0.8719262652671806\n",
            "Seq_len 5, epoch 4001, Loss: 0.0003464965848252177, deltaLoss: 0.06772406021896739\n",
            "Seq_len 5, epoch 5001, Loss: 0.0003551490372046828, deltaLoss: 0.02497124865986669\n",
            "Seq_len 5, epoch 6001, Loss: 3.6351124435896054e-05, deltaLoss: 0.8976454371888222\n",
            "Seq_len 5, epoch 7001, Loss: 0.0001448786206310615, deltaLoss: 2.9855334017672526\n",
            "Seq_len 5, epoch 8001, Loss: 0.0003345411387272179, deltaLoss: 1.3091132236766576\n",
            "Seq_len 5, epoch 9001, Loss: 0.000793961517047137, deltaLoss: 1.373285151320438\n",
            "Seq_len 5, epoch 10000, Loss: 0.0009396134410053492, deltaLoss: 0.18344960156244558\n",
            "Seq_len 5, epoch 1, Loss: 0.0009879095014184713, deltaLoss: 0.051399925017512844\n",
            "Seq_len 5, epoch 1001, Loss: 6.64925380533532e-07, deltaLoss: 0.9993269369516349\n",
            "Seq_len 5, epoch 2001, Loss: 1.9042798271584616e-07, deltaLoss: 0.7136099955110031\n",
            "Seq_len 5, epoch 3001, Loss: 6.657415241306808e-08, deltaLoss: 0.650397218604111\n",
            "Seq_len 5, epoch 4001, Loss: 2.6742629444242993e-08, deltaLoss: 0.5983031180282277\n",
            "Seq_len 5, epoch 5001, Loss: 1.1866325699827485e-08, deltaLoss: 0.5562767780719483\n",
            "Seq_len 5, epoch 6001, Loss: 5.714064776896066e-09, deltaLoss: 0.5184638512847208\n",
            "Seq_len 5, epoch 7001, Loss: 2.8949125319144287e-09, deltaLoss: 0.4933707185785576\n",
            "Seq_len 5, epoch 8001, Loss: 1.5465108083034806e-09, deltaLoss: 0.4657832348113258\n",
            "Seq_len 5, epoch 9001, Loss: 9.133930833904458e-10, deltaLoss: 0.4093846105140149\n",
            "Seq_len 5, epoch 10000, Loss: 5.660547586217035e-10, deltaLoss: 0.38027255853465497\n",
            "done alpha, tau, test loss 0.5 0.9 tensor(4.0804, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.3550409078598022, deltaLoss: 0.9986449590921402\n",
            "Seq_len 5, epoch 1001, Loss: 0.00029114645440131426, deltaLoss: 0.9997851382547106\n",
            "Seq_len 5, epoch 2001, Loss: 0.00023084749409463257, deltaLoss: 0.20710868841138633\n",
            "Seq_len 5, epoch 3001, Loss: 0.0008326382958330214, deltaLoss: 2.606876042118497\n",
            "Seq_len 5, epoch 4001, Loss: 0.0004552781756501645, deltaLoss: 0.4532101418723759\n",
            "Seq_len 5, epoch 5001, Loss: 0.0005359771894291043, deltaLoss: 0.17725210232995864\n",
            "Seq_len 5, epoch 6001, Loss: 1.834117756516207e-05, deltaLoss: 0.9657799288348481\n",
            "Seq_len 5, epoch 7001, Loss: 0.000442440650658682, deltaLoss: 23.122805042740037\n",
            "Seq_len 5, epoch 8001, Loss: 9.843430598266423e-05, deltaLoss: 0.7775197513245664\n",
            "Seq_len 5, epoch 9001, Loss: 0.00013603524712380022, deltaLoss: 0.3819902092646245\n",
            "Seq_len 5, epoch 10000, Loss: 0.0006494546541944146, deltaLoss: 3.7741645487170836\n",
            "Seq_len 5, epoch 1, Loss: 0.0009731674217619002, deltaLoss: 0.4984378285332635\n",
            "Seq_len 5, epoch 1001, Loss: 7.503301446831756e-08, deltaLoss: 0.9999228981439469\n",
            "Seq_len 5, epoch 2001, Loss: 5.178237927339069e-08, deltaLoss: 0.3098720657790495\n",
            "Seq_len 5, epoch 3001, Loss: 3.951902272092411e-08, deltaLoss: 0.2368248953513869\n",
            "Seq_len 5, epoch 4001, Loss: 3.091929556831019e-08, deltaLoss: 0.21760981321181888\n",
            "Seq_len 5, epoch 5001, Loss: 2.4600103287752972e-08, deltaLoss: 0.20437698092429651\n",
            "Seq_len 5, epoch 6001, Loss: 1.976951224946788e-08, deltaLoss: 0.19636466488699567\n",
            "Seq_len 5, epoch 7001, Loss: 1.60058561959886e-08, deltaLoss: 0.19037677844482906\n",
            "Seq_len 5, epoch 8001, Loss: 1.2903027979405124e-08, deltaLoss: 0.1938558099354353\n",
            "Seq_len 5, epoch 9001, Loss: 1.0413366169359506e-08, deltaLoss: 0.19295174853681132\n",
            "Seq_len 5, epoch 10000, Loss: 8.470087742296073e-09, deltaLoss: 0.18661385717726645\n",
            "done alpha, tau, test loss 0.5 1.0 tensor(4.6415, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 0.9620850682258606, deltaLoss: 0.9990379149317742\n",
            "Seq_len 5, epoch 1001, Loss: 3.567382736946456e-05, deltaLoss: 0.9999629202982692\n",
            "Seq_len 5, epoch 2001, Loss: 0.00022195256315171719, deltaLoss: 5.221719941990305\n",
            "Seq_len 5, epoch 3001, Loss: 5.653637708746828e-05, deltaLoss: 0.7452772057026327\n",
            "Seq_len 5, epoch 4001, Loss: 0.00046337596722878516, deltaLoss: 7.19606757808144\n",
            "Seq_len 5, epoch 5001, Loss: 0.0006458118441514671, deltaLoss: 0.3937102694680902\n",
            "Seq_len 5, epoch 6001, Loss: 8.303736103698611e-05, deltaLoss: 0.8714217433622807\n",
            "Seq_len 5, epoch 7001, Loss: 0.0023186095058918, deltaLoss: 26.922485456384575\n",
            "Seq_len 5, epoch 8001, Loss: 9.647291153669357e-05, deltaLoss: 0.9583919106293893\n",
            "Seq_len 5, epoch 9001, Loss: 0.0018038272392004728, deltaLoss: 17.697758888663635\n",
            "Seq_len 5, epoch 10000, Loss: 7.457448373315856e-05, deltaLoss: 0.9586576352144384\n",
            "Seq_len 5, epoch 1, Loss: 5.307610990712419e-05, deltaLoss: 0.288280558574962\n",
            "Seq_len 5, epoch 1001, Loss: 1.991090130104567e-06, deltaLoss: 0.9624861329590904\n",
            "Seq_len 5, epoch 2001, Loss: 1.4772891745451489e-06, deltaLoss: 0.2580500740729575\n",
            "Seq_len 5, epoch 3001, Loss: 1.1249027238591225e-06, deltaLoss: 0.23853586471621216\n",
            "Seq_len 5, epoch 4001, Loss: 8.664061965646397e-07, deltaLoss: 0.22979456073115145\n",
            "Seq_len 5, epoch 5001, Loss: 6.730473387506208e-07, deltaLoss: 0.22317344748998802\n",
            "Seq_len 5, epoch 6001, Loss: 5.254478310234845e-07, deltaLoss: 0.219300336290053\n",
            "Seq_len 5, epoch 7001, Loss: 4.130713477934478e-07, deltaLoss: 0.21386801237935665\n",
            "Seq_len 5, epoch 8001, Loss: 3.2583602660452016e-07, deltaLoss: 0.21118705437915966\n",
            "Seq_len 5, epoch 9001, Loss: 2.586151310879359e-07, deltaLoss: 0.206302833413117\n",
            "Seq_len 5, epoch 10000, Loss: 2.0588069560290023e-07, deltaLoss: 0.20391086655755097\n",
            "done alpha, tau, test loss 0.5 1.1 tensor(11.2929, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.5593880414962769, deltaLoss: 0.9984406119585038\n",
            "Seq_len 5, epoch 1001, Loss: 0.0006178751937113702, deltaLoss: 0.9996037707246245\n",
            "Seq_len 5, epoch 2001, Loss: 0.0003909356310032308, deltaLoss: 0.36729029586863515\n",
            "Seq_len 5, epoch 3001, Loss: 5.199310544412583e-05, deltaLoss: 0.8670034109945426\n",
            "Seq_len 5, epoch 4001, Loss: 0.00010594690684229136, deltaLoss: 1.0377106913943954\n",
            "Seq_len 5, epoch 5001, Loss: 0.0014102807035669684, deltaLoss: 12.31120223893143\n",
            "Seq_len 5, epoch 6001, Loss: 0.0001952288148459047, deltaLoss: 0.86156740686296\n",
            "Seq_len 5, epoch 7001, Loss: 7.418063614750281e-05, deltaLoss: 0.6200323389451806\n",
            "Seq_len 5, epoch 8001, Loss: 0.003730922006070614, deltaLoss: 49.295093164905545\n",
            "Seq_len 5, epoch 9001, Loss: 4.875452214037068e-05, deltaLoss: 0.9869323126934731\n",
            "Seq_len 5, epoch 10000, Loss: 0.0006113318959251046, deltaLoss: 11.538978316001122\n",
            "Seq_len 5, epoch 1, Loss: 0.0006259357323870063, deltaLoss: 0.02388855637869549\n",
            "Seq_len 5, epoch 1001, Loss: 5.094973857922014e-06, deltaLoss: 0.9918602284638834\n",
            "Seq_len 5, epoch 2001, Loss: 3.879292307829019e-06, deltaLoss: 0.23860408001952166\n",
            "Seq_len 5, epoch 3001, Loss: 3.3669341519271256e-06, deltaLoss: 0.13207516094311175\n",
            "Seq_len 5, epoch 4001, Loss: 3.0628652893938124e-06, deltaLoss: 0.09031030867035933\n",
            "Seq_len 5, epoch 5001, Loss: 2.8458537144615548e-06, deltaLoss: 0.07085247127378806\n",
            "Seq_len 5, epoch 6001, Loss: 2.676132680790033e-06, deltaLoss: 0.059638003460635844\n",
            "Seq_len 5, epoch 7001, Loss: 2.534783789087669e-06, deltaLoss: 0.0528183421984279\n",
            "Seq_len 5, epoch 8001, Loss: 2.411509740340989e-06, deltaLoss: 0.04863296399376504\n",
            "Seq_len 5, epoch 9001, Loss: 2.3002896796242567e-06, deltaLoss: 0.046120510672706554\n",
            "Seq_len 5, epoch 10000, Loss: 2.1986918454786064e-06, deltaLoss: 0.04416740858579424\n",
            "done alpha, tau, test loss 0.5 1.2 tensor(12.4206, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.3152920007705688, deltaLoss: 0.9986847079992295\n",
            "Seq_len 5, epoch 1001, Loss: 0.037195608019828796, deltaLoss: 0.9717206460633551\n",
            "Seq_len 5, epoch 2001, Loss: 0.0381033830344677, deltaLoss: 0.024405435559891112\n",
            "Seq_len 5, epoch 3001, Loss: 0.03746480867266655, deltaLoss: 0.016758993846386384\n",
            "Seq_len 5, epoch 4001, Loss: 0.037225838750600815, deltaLoss: 0.006378517081286518\n",
            "Seq_len 5, epoch 5001, Loss: 0.03751068934798241, deltaLoss: 0.007651959148321255\n",
            "Seq_len 5, epoch 6001, Loss: 0.03763807937502861, deltaLoss: 0.0033960993322308952\n",
            "Seq_len 5, epoch 7001, Loss: 0.03804146870970726, deltaLoss: 0.010717585524469745\n",
            "Seq_len 5, epoch 8001, Loss: 0.03820252791047096, deltaLoss: 0.0042337797731401465\n",
            "Seq_len 5, epoch 9001, Loss: 0.03748113289475441, deltaLoss: 0.018883436651290948\n",
            "Seq_len 5, epoch 10000, Loss: 0.03721268102526665, deltaLoss: 0.00716232004623673\n",
            "Seq_len 5, epoch 1, Loss: 0.037452083081007004, deltaLoss: 0.0064333460837666424\n",
            "Seq_len 5, epoch 1001, Loss: 0.03705994784832001, deltaLoss: 0.010470318348884022\n",
            "Seq_len 5, epoch 2001, Loss: 0.03705969452857971, deltaLoss: 6.835404661987229e-06\n",
            "Seq_len 5, epoch 2001, Loss: 0.03705969452857971, deltaLoss: 6.835404661987229e-06\n",
            "done alpha, tau, test loss 0.5 1.4 tensor(13.3129, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 0.9862573742866516, deltaLoss: 0.9990137426257133\n",
            "Seq_len 5, epoch 1001, Loss: 0.08352073282003403, deltaLoss: 0.9153154795111736\n",
            "Seq_len 5, epoch 2001, Loss: 0.08426182717084885, deltaLoss: 0.008873178261159293\n",
            "Seq_len 5, epoch 3001, Loss: 0.0835709273815155, deltaLoss: 0.008199439918772217\n",
            "Seq_len 5, epoch 4001, Loss: 0.08353903889656067, deltaLoss: 0.00038157390319791025\n",
            "Seq_len 5, epoch 5001, Loss: 0.08376269787549973, deltaLoss: 0.0026772989238719206\n",
            "Seq_len 5, epoch 6001, Loss: 0.08366484940052032, deltaLoss: 0.001168162887074593\n",
            "Seq_len 5, epoch 7001, Loss: 0.08360626548528671, deltaLoss: 0.000700221367197581\n",
            "Seq_len 5, epoch 8001, Loss: 0.08363057672977448, deltaLoss: 0.0002907825669122946\n",
            "Seq_len 5, epoch 9001, Loss: 0.08362117409706116, deltaLoss: 0.00011243056165569058\n",
            "Seq_len 5, epoch 10000, Loss: 0.08366034924983978, deltaLoss: 0.000468483649047476\n",
            "Seq_len 5, epoch 1, Loss: 0.08356507867574692, deltaLoss: 0.0011387781063207484\n",
            "Seq_len 5, epoch 1001, Loss: 0.08351831883192062, deltaLoss: 0.000559562015225686\n",
            "Seq_len 5, epoch 2001, Loss: 0.0835181400179863, deltaLoss: 2.1410145322253465e-06\n",
            "Seq_len 5, epoch 2001, Loss: 0.0835181400179863, deltaLoss: 2.1410145322253465e-06\n",
            "done alpha, tau, test loss 0.5 1.6 tensor(5.8986, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 2.3916757106781006, deltaLoss: 0.9976083242893219\n",
            "Seq_len 5, epoch 1001, Loss: 0.17150795459747314, deltaLoss: 0.9282896281332195\n",
            "Seq_len 5, epoch 2001, Loss: 0.17166267335414886, deltaLoss: 0.0009021083426645898\n",
            "Seq_len 5, epoch 3001, Loss: 0.17155633866786957, deltaLoss: 0.0006194397663837087\n",
            "Seq_len 5, epoch 4001, Loss: 0.17173583805561066, deltaLoss: 0.0010462999451661004\n",
            "Seq_len 5, epoch 5001, Loss: 0.1715487241744995, deltaLoss: 0.0010895447521591546\n",
            "Seq_len 5, epoch 6001, Loss: 0.17154429852962494, deltaLoss: 2.579818005566153e-05\n",
            "Seq_len 5, epoch 6001, Loss: 0.17154429852962494, deltaLoss: 2.579818005566153e-05\n",
            "done alpha, tau, test loss 0.5 1.9 tensor(3.5651, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.6330602169036865, deltaLoss: 0.9983669397830963\n",
            "Seq_len 5, epoch 1001, Loss: 0.2572108507156372, deltaLoss: 0.8424976323265568\n",
            "Seq_len 5, epoch 2001, Loss: 0.2571958601474762, deltaLoss: 5.8281243265214186e-05\n",
            "Seq_len 5, epoch 2001, Loss: 0.2571958601474762, deltaLoss: 5.8281243265214186e-05\n",
            "done alpha, tau, test loss 0.5 2.2 tensor(3.6721, device='cuda:0')\n",
            "Seq_len 5, epoch 1, Loss: 1.8096240758895874, deltaLoss: 0.9981903759241104\n",
            "Seq_len 5, epoch 1001, Loss: 0.3800289034843445, deltaLoss: 0.7899956634377074\n",
            "Seq_len 5, epoch 2001, Loss: 0.38002967834472656, deltaLoss: 2.038951182332896e-06\n",
            "Seq_len 5, epoch 2001, Loss: 0.38002967834472656, deltaLoss: 2.038951182332896e-06\n",
            "done alpha, tau, test loss 0.5 2.8 tensor(2.2207, device='cuda:0')\n",
            "[[10.0565033   2.18771744  3.09038925  3.6806376   4.08040619  4.64145803\n",
            "  11.29286194 12.42056465 13.31289673  5.89860821  3.56511784  3.67212319\n",
            "   2.22066116]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercise 4: Define a function that provides theoretical estimate of Generalization Error for this in-context learning task.**\n",
        "> Hint: Generalization error is the test loss."
      ],
      "metadata": {
        "id": "TpyRNX8l10OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Solution:**"
      ],
      "metadata": {
        "id": "m6-5ZNxo3WdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given below is the theoretical test loss.\n",
        "\n",
        "def TheoryTestLoss(sigma_epsilon, sigma_omega, alpha, tau):\n",
        "  rho = sigma_epsilon/sigma_omega\n",
        "\n",
        "  if tau < 1:\n",
        "    loss = alpha*(1-tau)/(1+alpha+rho**2)+((1+rho**2)**2+alpha*rho**2)/(1+alpha+rho**2)/(1-tau)\n",
        "  else:\n",
        "    loss = ((1+rho**2)**2+alpha*rho**2)/(1+alpha+rho**2)*tau/(tau-1)\n",
        "\n",
        "  return (sigma_omega**2)*loss"
      ],
      "metadata": {
        "id": "FSbMZr5R3YnM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercise 5: For each point in the following array, compute Generalization error using the funciton just defined. Then plot the empirical test loss and theoretical generalization error of this Linearized Transformer for comparison.**\n",
        "\n",
        "\n",
        "```\n",
        "t_list = np.linspace(0.1, 2.8, num=80)\n",
        "t_array = np.zeros(len(t_list))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yj7F9ogw3gHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute theoretical test loss or generalization error\n",
        "\n",
        "t_list = np.linspace(0.1, 2.8, num=80)\n",
        "t_array = np.zeros(len(t_list))\n",
        "\n",
        "for t in range(len(t_list)):\n",
        "  t_array[t] = TheoryTestLoss(0.1, 1, 0.5, t_list[t])\n",
        "\n",
        "# Plot empirical test loss vs theoretical generalization error\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(tau_list, test_loss_list, linestyle='', marker='o', color='b', label='Empricial Test loss')\n",
        "plt.plot(t_list, t_array, color='r', label='Theoretical Test loss')\n",
        "plt.title(r'Test Loss at $\\alpha=0.5$ and $d=10$, $K=Q=V=\\frac{\\mathbf{1}}{\\sqrt{d+1}}$, linear transformer')\n",
        "plt.xlabel(r'$\\tau$')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "3OfV6gOSdOO3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "673ae358-d876-4308-e74f-f54ec14a8b7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment: If you increase token dimension, e.g. $d=2000$ or $d=500$ at home / cluster, you will see a good match between theory and empirical generalization errors. This is because sequence length $l$ also increase in proportion, while sample size $p$ increases quadratically. Please feel free to play with the token dimension. This exaplains why Linearized Transformers (extrapolating this to real life nonlinear transformers) generalize so well when trained over large data.**"
      ],
      "metadata": {
        "id": "BtdI4Mle4Xqk"
      }
    }
  ]
}